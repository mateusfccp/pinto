import 'package:collection/collection.dart';
import 'package:pinto/error.dart';

import 'token.dart';

/// A Pinto lexer.
final class Lexer {
  /// Creates a Pinto lexer for [source].
  Lexer({
    required String source,
    ErrorHandler? errorHandler,
  })  : _errorHandler = errorHandler,
        _source = source;

  final String _source;
  final ErrorHandler? _errorHandler;
  final _tokens = <Token>[];
  final _lineBreaks = <int>[];

  var _start = 0;
  var _current = 0;

  // TODO(mateusfccp): Check what should be a contextual keyword or not
  static const _keywords = {
    'import': TokenType.importKeyword,
    'false': TokenType.falseKeyword,
    'let': TokenType.letKeyword, // TODO: Make it a contextual keyword for iteroperability reasons
    'true': TokenType.trueKeyword,
    'type': TokenType.typeKeyword,

    // Reserved for interoperability with Dart
    'assert': TokenType.reserved,
    'break': TokenType.reserved,
    'case': TokenType.reserved,
    'catch': TokenType.reserved,
    'class': TokenType.reserved,
    'const': TokenType.reserved,
    'continue': TokenType.reserved,
    'default': TokenType.reserved,
    'do': TokenType.reserved,
    'else': TokenType.reserved,
    'enum': TokenType.reserved,
    'extends': TokenType.reserved,
    'final': TokenType.reserved,
    'finally': TokenType.reserved,
    'for': TokenType.reserved,
    'if': TokenType.reserved,
    'in': TokenType.reserved,
    'is': TokenType.reserved,
    'new': TokenType.reserved,
    'null': TokenType.reserved,
    'rethrow': TokenType.reserved,
    'return': TokenType.reserved,
    'super': TokenType.reserved,
    'switch': TokenType.reserved,
    'this': TokenType.reserved,
    'throw': TokenType.reserved,
    'try': TokenType.reserved,
    'var': TokenType.reserved,
    'void': TokenType.reserved,
    'with': TokenType.reserved,
    'while': TokenType.reserved,
  };

  bool get _isAtEnd => _current >= _source.length;

  /// Scans the source and returns the tokens.
  List<Token> scanTokens() {
    while (!_isAtEnd) {
      _start = _current;
      _scanToken();
    }

    _tokens.add(
      Token(
        type: TokenType.endOfFile,
        lexeme: '',
        offset: _current,
      ),
    );
    return _tokens;
  }

  (int line, int column) positionForOffset(int offset) {
    final bound = lowerBound(_lineBreaks, offset);
    final line = bound + 1;

    final start = bound == 0 ? 0 : _lineBreaks[bound - 1];
    final column = offset - start;

    print('Offset: $offset, Line: $line');

    return (line, column);
  }

  void _scanToken() {
    final character = _advance();
    return switch (character) {
      '⊤' => _addToken(TokenType.verum),
      '⊥' => _addToken(TokenType.falsum),
      '@' => _identifier(true),
      '?' => _addToken(TokenType.eroteme),
      '(' => _leftParenthesis(),
      ')' => _addToken(TokenType.rightParenthesis),
      '[' => _addToken(TokenType.leftBracket),
      ']' => _addToken(TokenType.rightBracket),
      '{' => _addToken(TokenType.leftBrace),
      '}' => _addToken(TokenType.rightBrace),
      ',' => _addToken(TokenType.comma),
      '+' => _addToken(TokenType.plusSign),
      '=' => _addToken(TokenType.equalitySign),
      ':' => _addToken(TokenType.colon),
      '"' => _string(),
      '/' => _slash(),
      ' ' || '\r' || '\t' => null,
      '\n' => _lineBreak(),
      final character => _character(character),
    };
  }

  void _leftParenthesis() {
    if (_match(')')) {
      _addToken(TokenType.unitLiteral);
    } else {
      _addToken(TokenType.leftParenthesis);
    }
  }

  void _slash() {
    if (_match('/')) {
      while (_peek != '\n' && !_isAtEnd) {
        _advance();
      }
    } else if (_match('*')) {
      _advanceUntilCommentEnd();
    } else {
      _addToken(TokenType.slash);
    }
  }

  void _string() {
    while (_peek != '"' && _peek != '\n' && !_isAtEnd) {
      _advance();
    }

    // Strings should not span to more than one line, and should be closed
    if (_peek == '\n' || _isAtEnd) {
      _errorHandler?.emit(
        UnterminatedStringError(offset: _current),
      );
      _lineBreak();
    } else {
      // Advance to the closing quotes (")
      _advance();
      _addToken(TokenType.stringLiteral);
    }
  }

  void _lineBreak() => _lineBreaks.add(_current - 1);

  void _character(String character) {
    if (_isIdentifierStart(character)) {
      _identifier(false);
    } else {
      _errorHandler?.emit(
        UnexpectedCharacterError(offset: _current - 1),
      );
    }
  }

  void _addToken(TokenType type) {
    final text = _source.substring(_start, _current);

    final token = Token(
      type: type,
      lexeme: text,
      offset: _start,
    );

    _tokens.add(token);
  }

  String _advance() => _source[_current++];

  void _advanceUntilCommentEnd() {
    int commentLevel = 1;

    while (commentLevel > 0 && !_isAtEnd) {
      if (_match('/') && _peek == '*') {
        commentLevel = commentLevel + 1;
      } else if (_match('*') && _peek == '/') {
        commentLevel = commentLevel - 1;
      }

      if (_peek == '\n') {
        _lineBreak();
      }

      _advance();
    }
  }

  bool _match(String expected) {
    if (_isAtEnd || _source[_current] != expected) {
      return false;
    } else {
      _current++;
      return true;
    }
  }

  String get _peek => _isAtEnd ? '\x00' : _source[_current];

  bool _isIdentifierStart(String character) {
    assert(character.length == 1);
    return RegExp(r'[A-Za-z_$]').hasMatch(character);
  }

  bool _isIdentifierPart(String character) {
    assert(character.length == 1);
    return RegExp(r'[A-Za-z_$0-9]').hasMatch(character);
  }

  void _identifier(bool isImportIdentifier) {
    while (_isIdentifierPart(_peek)) {
      _advance();
    }

    while (_peek == '/') {
      isImportIdentifier = true;

      _advance();

      while (_isIdentifierPart(_peek)) {
        _advance();
      }
    }

    final text = _source.substring(_start, _current);
    final TokenType tokenType;

    if (isImportIdentifier) {
      tokenType = TokenType.importIdentifier;
    } else if (_keywords[text] case final keyword?) {
      tokenType = keyword;
    } else {
      tokenType = TokenType.identifier;
    }

    _addToken(tokenType);
  }
}
